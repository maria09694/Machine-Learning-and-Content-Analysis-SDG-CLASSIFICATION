{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SDG Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "bK-0u31sZlgl",
    "outputId": "697dad2b-a813-4eb7-cbed-bd0e9ab77060"
   },
   "outputs": [],
   "source": [
    "#Our first step is to import the data into Python.\n",
    "\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAzjrkpsZlgs"
   },
   "outputs": [],
   "source": [
    "#By executing the following code, we will extract our data in a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE BELOW CODE IS COMMENTED OUT FOR YOUR CONVENIENCE. YOU CAN SKIP THIS STEP, AS THE FINAL CSV \n",
    "#IS INCLUDED IN THE GITHUB DELIVERIES AND IT IS IMPORTED IN THE NEXT CODE CHUNK.\n",
    "\n",
    "#import os\n",
    "#import re\n",
    "#from pathlib import Path\n",
    "\n",
    "#import pandas as pd\n",
    "#from pprint import pprint\n",
    "\n",
    "#pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "\n",
    "#class SGDParser:\n",
    "\n",
    "#    def __init__(self):\n",
    "#        pass\n",
    "\n",
    "#    @staticmethod\n",
    "#    def get_sdg_files():\n",
    "#        return list(Path('sdg').glob('**/*.txt'))\n",
    "\n",
    "#    @staticmethod\n",
    "#    def get_lines(fname: Path):\n",
    "#        \"\"\"\n",
    "\n",
    "#        :param fname:\n",
    "#        :return:\n",
    "#        \"\"\"\n",
    "#        with open(fname, 'r', encoding='utf-8-sig') as infile:\n",
    "#            lines = list(map(str.strip, infile.readlines()))\n",
    "#\n",
    "#        return lines\n",
    "\n",
    "#    @staticmethod\n",
    "#    def extract_sdg(fpath):\n",
    "#       \"\"\"\n",
    "\n",
    "#        :param fpath:\n",
    "#        :return:\n",
    "#        \"\"\"\n",
    "#        return str(fpath).split(os.sep)[-2]\n",
    "\n",
    "#    @property\n",
    "#    def keywords(self):\n",
    "#        \"\"\"\n",
    "\n",
    "#        :return:\n",
    "#        \"\"\"\n",
    "#        return sorted(\n",
    "#            set(map(str.lower, ['abstarct', 'abstract', 'abstratc', 'aim',\n",
    "#                                'background', 'introduction', 'objective',\n",
    "#                                'purpose']\n",
    "#                    )\n",
    "#                )\n",
    "#        )\n",
    "\n",
    "#    @staticmethod\n",
    "#    def lines_2_lowercase(lines):\n",
    "#        \"\"\"\n",
    "\n",
    "#        :param lines:\n",
    "#        :return:\n",
    "#        \"\"\"\n",
    "#        return list(map(str.lower, lines))\n",
    "\n",
    "#    @staticmethod\n",
    "#    def remove_title_prefix(title):\n",
    "#       \"\"\"\n",
    "\n",
    "#        :param title:\n",
    "#        :return:\n",
    "#        \"\"\"\n",
    "#        if title and title.startswith('Title:'):\n",
    "#            return re.sub('Title:', '', title).strip()\n",
    "\n",
    "#        return title\n",
    "\n",
    "#    def search_with_keyword(self, term, lowered_lines, lines, index):\n",
    "#        \"\"\"\n",
    "\n",
    "#        :param term:\n",
    "#        :param lowered_lines:\n",
    "#        :param lines:\n",
    "#        :param index:\n",
    "#        :return:\n",
    "#        \"\"\"\n",
    "#        doc = dict()\n",
    "#        matched = False\n",
    "\n",
    "#        if lowered_lines[index].startswith(term):\n",
    "#            doc['extracted_title'] = self.join_lines(lines[:index])\n",
    "#            doc['extracted_abstract'] = self.join_lines(lines[index:])\n",
    "#            doc['extracted'] = True\n",
    "#            matched = True\n",
    "\n",
    "#        return doc, matched\n",
    "\n",
    "#    @staticmethod\n",
    "#    def join_lines(lines):\n",
    "#        return ' || '.join(lines)\n",
    "\n",
    "#    def search_in_lines(self, lowered_lines, lines, index):\n",
    "#        \"\"\"\n",
    "\n",
    "#        :param lowered_lines:\n",
    "#        :param lines:\n",
    "#        :param index:\n",
    "#        :return:\n",
    "#        \"\"\"\n",
    "#        doc = dict()\n",
    "#        matched = False\n",
    "\n",
    "#        for word in self.keywords:\n",
    "#            if lowered_lines[index].startswith(word):\n",
    "#                doc['extracted_title'] = self.join_lines(lines[:index])\n",
    "#                doc['extracted_abstract'] = self.join_lines(lines[index:])\n",
    "#                doc['extracted'] = True\n",
    "#                matched = True\n",
    "#                break\n",
    "\n",
    "#        return doc, matched\n",
    "\n",
    "#   def create_sdg_dataset(self):\n",
    "#        \"\"\"\n",
    "\n",
    "#        :return:\n",
    "#        \"\"\"\n",
    "#        files = self.get_sdg_files()\n",
    "\n",
    "#        data = list()\n",
    "\n",
    "#        for file in files:\n",
    "\n",
    "#            lines = self.get_lines(file)\n",
    "\n",
    "#            matched = False\n",
    "\n",
    "#            lowered_lines = self.lines_2_lowercase(lines)\n",
    "\n",
    "#            doc = {'sdg': self.extract_sdg(file),\n",
    "#                   'extracted_title': None,\n",
    "#                   'extracted_abstract': None,\n",
    "#                   'initial_text': ' || '.join(lines)}\n",
    "#\n",
    "#            if 'abstract' in lowered_lines:\n",
    "#                abs_index = lowered_lines.index('abstract')\n",
    "#                title_lines = lines[:abs_index]\n",
    "#                abstract_lines = lines[abs_index + 1:]\n",
    "#\n",
    "#                doc['extracted_title'] = self.join_lines(title_lines)\n",
    "#                doc['extracted_abstract'] = self.join_lines(abstract_lines)\n",
    "#                doc['extracted'] = True\n",
    "#               matched = True\n",
    "#\n",
    "#            if not matched:\n",
    "#                temp_doc, matched = self.search_in_lines(\n",
    "#                    lowered_lines, lines, 1)\n",
    "#                doc.update(temp_doc)\n",
    "#\n",
    "#            if not matched:\n",
    "#                temp_doc, matched = self.search_in_lines(\n",
    "#                    lowered_lines, lines, 2)\n",
    "#                doc.update(temp_doc)\n",
    "#\n",
    "#            if not matched:\n",
    "#                temp_doc, matched = self.search_in_lines(\n",
    "#                    lowered_lines, lines, 3)\n",
    "#                doc.update(temp_doc)\n",
    "\n",
    "#            key_phrases = ['this study', 'in this study', 'the study',\n",
    "#                           'this paper', 'in this paper', 'this research',\n",
    "#                           'the objective', 'the aim', 'this report',\n",
    "#                           'the report', 'the purpose of', 'the paper']\n",
    "\n",
    "#            for key_phrase in key_phrases:\n",
    "#                if not matched:\n",
    "#                    temp_doc, matched = self.search_with_keyword(\n",
    "#                        key_phrase, lowered_lines, lines, 1)\n",
    "#                    doc.update(temp_doc)\n",
    "#\n",
    "#            for key_phrase in key_phrases:\n",
    "#                if not matched:\n",
    "#                    temp_doc, matched = self.search_with_keyword(\n",
    "#                        key_phrase, lowered_lines, lines, 2)\n",
    "#                    doc.update(temp_doc)\n",
    "\n",
    "#            if not matched:\n",
    "#                if lowered_lines[0].startswith('title:'):\n",
    "#                    doc['extracted_title'] = lines[0]\n",
    "#                    doc['extracted_abstract'] = self.join_lines(lines[1:])\n",
    "#                    doc['extracted'] = True\n",
    "#\n",
    "#                else:\n",
    "#                    doc['extracted_title'] = lines[0]\n",
    "#                    doc['extracted_abstract'] = self.join_lines(lines[1:])\n",
    "#                    doc['extracted'] = False\n",
    "\n",
    "#            doc['extracted_title'] = self.remove_title_prefix(\n",
    "#                doc['extracted_title'])\n",
    "\n",
    "#            data.append(doc)\n",
    "\n",
    "#        return data\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    parser = SGDParser()\n",
    "#    sdg_df = pd.DataFrame(parser.create_sdg_dataset())\n",
    "#    print(sdg_df['sdg'].value_counts())\n",
    "#    print(sdg_df['extracted'].value_counts())\n",
    "#    print(sdg_df)\n",
    "    #sdg_df.to_csv('sgd_papers.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sdg_df = pd.read_csv(path + '/in/sgd_papers.csv')\n",
    "list(sdg_df.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at our data\n",
    "sdg_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's confirm that there are not NAs\n",
    "sdg_df.isnull().values.any()  #True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: \"https://stackoverflow.com/questions/44548721/remove-row-with-null-value-from-pandas-data-frame/44548976\"\n",
    "#It will erase every row (axis=0) that has \"any\" Null value in it.\n",
    "sdg_df = sdg_df.dropna(how='any',axis=0)\n",
    "\n",
    "#Just to check\n",
    "sdg_df.isnull().values.any() #False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at our data again\n",
    "sdg_df.shape #only 1 row with null values was deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will take a look at our data by:\n",
    "#using .info() to get the types of the varibles\n",
    "#and .head() to get the first rows of the data frame\n",
    "sdg_df.info()\n",
    "sdg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will create new columns in order to apply our methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will make new columns from the initial ones as we remove all punctuations from each column\n",
    "sdg_df[\"extracted_title_new\"] = sdg_df['extracted_title'].str.replace('[^\\w\\s]','')\n",
    "sdg_df[\"extracted_abstract_new\"] = sdg_df['extracted_abstract'].str.replace('[^\\w\\s]','')\n",
    "sdg_df[\"initial_text_new\"] = sdg_df['initial_text'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(sdg_df['initial_text_new']) #see the data without punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now compute Word Tokenization\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: ''https://stackoverflow.com/questions/33098040/how-to-use-word-tokenize-in-data-frame''\n",
    "#Word tokenization\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk import punkt\n",
    "sdg_df['extracted_title_new'] = sdg_df.apply(lambda row: nltk.word_tokenize(row['extracted_title_new']), axis=1)\n",
    "sdg_df['extracted_abstract_new'] = sdg_df.apply(lambda row: nltk.word_tokenize(row['extracted_abstract_new']), axis=1)\n",
    "sdg_df['initial_text_new'] = sdg_df.apply(lambda row: nltk.word_tokenize(row['initial_text_new']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(sdg_df['initial_text_new']) #see the tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now remove stopwords from each column\n",
    "#Source: ''https://pythonhealthcare.org/2018/12/14/101-pre-processing-data-tokenization-stemming-and-removal-of-stop-words/''\n",
    "#Remove all English stopwords:\n",
    "nltk.download(u'stopwords')\n",
    "#import stopwords from nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))  #make a list, \"stop\", with english stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'extracted_title_new' column\n",
    "def remove_stops(row):\n",
    "    my_list = row['extracted_title_new']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "sdg_df['extracted_title_new'] = sdg_df.apply(remove_stops, axis=1)\n",
    "print(sdg_df['extracted_title_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'extracted_abstract_new' column\n",
    "def remove_stops(row):\n",
    "    my_list = row['extracted_abstract_new']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "sdg_df['extracted_abstract_new'] = sdg_df.apply(remove_stops, axis=1)\n",
    "print(sdg_df['extracted_abstract_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'initial_text_new' column\n",
    "def remove_stops(row):\n",
    "    my_list = row['initial_text_new']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "sdg_df['initial_text_new'] = sdg_df.apply(remove_stops, axis=1)\n",
    "print(sdg_df['initial_text_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR YOUR CONVENIENCE, YOU CAN SKIP THIS STEP, AS THE FINAL PROCESSED CSV WITH THE NORMALIZED COLUMNS \n",
    "#IS INCLUDED IN THE GITHUB DELIVERIES AND IT IS IMPORTED IN THE NEXT CODE CHUNK.\n",
    "\n",
    "\n",
    "#POS tagger\n",
    "\n",
    "#First, the user must download the file from the following link and place it in their working directory:\n",
    "#https://nlp.stanford.edu/software/tagger.html\n",
    "\n",
    "#from nltk.tag import StanfordPOSTagger\n",
    "#gs_path = os.path.join(os.getcwd(),\n",
    "#                       'stanford-tagger-4.1.0',\n",
    "#                       'stanford-postagger-full-2020-08-06',\n",
    "#                       'stanford-postagger.jar')\n",
    "#jar_path = os.path.join(os.getcwd(),\n",
    "#                        'stanford-tagger-4.1.0',\n",
    "#                        'stanford-postagger-full-2020-08-06',\n",
    "#                        'models',\n",
    "#                        'english-left3words-distsim.tagger')\n",
    "#pos_tagger = StanfordPOSTagger(gs_path, jar_path, encoding='utf8')\n",
    "\n",
    "#Source: ''https://github.com/royn5618/Medium_Blog_Codes/blob/master/Text_Normalization_ft_POS_Tagger.ipynb?fbclid=IwAR16d1Vui2YZN9QQ9KfAQwL-N38kp9jLiwg5oPEy6pzh0hIX_d7-Nza4T_E''\n",
    "\n",
    "#create a mapper for the arguments to wordnet according to the treebank POS tag codes\n",
    "\n",
    "#from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n",
    "\n",
    "#Let's create a mapper:\n",
    "#dict_pos_map = {\n",
    "    # Look for NN in the POS tag because all nouns begin with NN\n",
    "#   'NN': NOUN,\n",
    "    # Look for VB in the POS tag because all nouns begin with VB\n",
    "#    'VB':VERB,\n",
    "    # Look for JJ in the POS tag because all nouns begin with JJ\n",
    "#    'JJ' : ADJ,\n",
    "    # Look for RB in the POS tag because all nouns begin with RB\n",
    "#    'RB':ADV  \n",
    "#}\n",
    "\n",
    "#Source: ''https://stackoverflow.com/questions/46202097/nltk-was-unable-to-find-the-java-file-for-stanford-pos-tagger''\n",
    "\n",
    "#Get the lemmas accoridngly for extracted_title_new (NO STEMMER)\n",
    "\n",
    "#from nltk.stem import  WordNetLemmatizer\n",
    "\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#import os\n",
    "#java_path = \"C:/Program Files (x86)/Common Files/Oracle/Java/javapath/java.exe\" #place your path to java.exe\n",
    "#os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "#normalized_sequence = []\n",
    "#for each_seq in pos_tagger.tag_sents(sentences=sdg_df['extracted_title_new']):\n",
    "#    normalized_tokens = []\n",
    "#    for tuples in each_seq:\n",
    "#        temp = tuples[0]\n",
    "#        if tuples[1] == \"NNP\" or tuples[1] == \"NNPS\":\n",
    "#            continue\n",
    "#        if tuples[1][:2] in dict_pos_map.keys():\n",
    "#            temp = lemmatizer.lemmatize(tuples[0].lower(), \n",
    "#                                pos=dict_pos_map[tuples[1][:2]])\n",
    "#        normalized_tokens.append(temp)\n",
    "#    normalized_sequence.append(normalized_tokens)\n",
    "#normalized_sequence\n",
    "\n",
    "#normalized_sequence = []\n",
    "#for each_seq in pos_tagger.tag_sents(sentences=sdg_df['extracted_title_new']):\n",
    "#    normalized_tokens = []\n",
    "#    for tuples in each_seq:\n",
    "#        temp = tuples[0]\n",
    "#        if tuples[1] == \"NNP\" or tuples[1] == \"NNPS\":\n",
    "#            continue\n",
    "#        if tuples[1][:2] in dict_pos_map.keys():\n",
    "#            temp = lemmatizer.lemmatize(tuples[0].lower(), \n",
    "#                                pos=dict_pos_map[tuples[1][:2]])\n",
    "#        normalized_tokens.append(temp)\n",
    "#    normalized_sequence.append(normalized_tokens)\n",
    "#normalized_sequence\n",
    "\n",
    "#Get the lemmas accoridngly for extracted_abstract_new (NO STEMMER)\n",
    "\n",
    "#from nltk.stem import  WordNetLemmatizer\n",
    "\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#import os\n",
    "#java_path = \"C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath\\java.exe\" #place your path to java.exe\n",
    "#os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "#normalized_sequence = []\n",
    "#for each_seq in pos_tagger.tag_sents(sentences=sdg_df['extracted_abstract_new']):\n",
    "#    normalized_tokens = []\n",
    "#    for tuples in each_seq:\n",
    "#        temp = tuples[0]\n",
    "#        if tuples[1] == \"NNP\" or tuples[1] == \"NNPS\":\n",
    "#            continue\n",
    "#        if tuples[1][:2] in dict_pos_map.keys():\n",
    "#            temp = lemmatizer.lemmatize(tuples[0].lower(), \n",
    "#                                pos=dict_pos_map[tuples[1][:2]])\n",
    "#        normalized_tokens.append(temp)\n",
    "#    normalized_sequence.append(normalized_tokens)\n",
    "#normalized_sequence\n",
    "\n",
    "#from nltk.internals import find_jars_within_path\n",
    "#from nltk.parse.stanford import StanfordParser\n",
    "#parser = StanfordParser(model_path=\"path/to/englishPCFG.ser.gz\")\n",
    "#parser._classpath = tuple(find_jars_within_path(stanford_dir))\n",
    "\n",
    "#Adding stemmer for extracted_title_new\n",
    "\n",
    "#from nltk.stem import PorterStemmer\n",
    "\n",
    "#stemmer = PorterStemmer()\n",
    "\n",
    "#normalized_extracted_title = []\n",
    "#for each_seq in pos_tagger.tag_sents(sentences=sdg_df['extracted_title_new']):\n",
    "#    normalized_tokens = []\n",
    "#    for tuples in each_seq:\n",
    "#        temp = tuples[0]\n",
    "#        if tuples[1] == \"NNP\" or tuples[1] == \"NNPS\":\n",
    "#            continue\n",
    "#        if tuples[1][:2] in dict_pos_map.keys():\n",
    "#            temp = lemmatizer.lemmatize(tuples[0].lower(), \n",
    "#                                pos=dict_pos_map[tuples[1][:2]])\n",
    "#        temp = stemmer.stem(temp)\n",
    "#        normalized_tokens.append(temp)\n",
    "#    normalized_extracted_title.append(normalized_tokens)\n",
    "#normalized_extracted_title\n",
    "\n",
    "#Add the resulting list as a column in our dataframe\n",
    "#sdg_df['normalized_extracted_title'] = normalized_extracted_title\n",
    "\n",
    "#Adding stemmer for extracted_abstract_new\n",
    "\n",
    "#from nltk.stem import PorterStemmer\n",
    "\n",
    "#stemmer = PorterStemmer()\n",
    "\n",
    "#normalized_extracted_abstract = []\n",
    "#for each_seq in pos_tagger.tag_sents(sentences=sdg_df['extracted_abstract_new']):\n",
    "#    normalized_tokens = []\n",
    "#    for tuples in each_seq:\n",
    "#        temp = tuples[0]\n",
    "#        if tuples[1] == \"NNP\" or tuples[1] == \"NNPS\":\n",
    "#            continue\n",
    "#        if tuples[1][:2] in dict_pos_map.keys():\n",
    "#            temp = lemmatizer.lemmatize(tuples[0].lower(), \n",
    "#                                pos=dict_pos_map[tuples[1][:2]])\n",
    "#        temp = stemmer.stem(temp)\n",
    "#        normalized_tokens.append(temp)\n",
    "#    normalized_extracted_abstract.append(normalized_tokens)\n",
    "#normalized_extracted_abstract\n",
    "\n",
    "#Add the resulting list as a column in our dataframe\n",
    "#sdg_df['normalized_extracted_abstract'] = normalized_extracted_abstract\n",
    "\n",
    "#Adding stemmer for initial_text_new\n",
    "\n",
    "#from nltk.stem import PorterStemmer\n",
    "\n",
    "#stemmer = PorterStemmer()\n",
    "\n",
    "#normalized_initial_text = []\n",
    "#for each_seq in pos_tagger.tag_sents(sentences=sdg_df['initial_text_new']):\n",
    "#    normalized_tokens = []\n",
    "#    for tuples in each_seq:\n",
    "#        temp = tuples[0]\n",
    "#        if tuples[1] == \"NNP\" or tuples[1] == \"NNPS\":\n",
    "#            continue\n",
    "#        if tuples[1][:2] in dict_pos_map.keys():\n",
    "#            temp = lemmatizer.lemmatize(tuples[0].lower(), \n",
    "#                                pos=dict_pos_map[tuples[1][:2]])\n",
    "#        temp = stemmer.stem(temp)\n",
    "#        normalized_tokens.append(temp)\n",
    "#    normalized_initial_text.append(normalized_tokens)\n",
    "#normalized_initial_text\n",
    "\n",
    "#Add the resulting list as a column in our dataframe\n",
    "#sdg_df['normalized_initial_text'] = normalized_initial_text\n",
    "\n",
    "#Let's take a look at our cleaned data via Excel\n",
    "#sdg_df.to_excel('sgd_papers2.xlsx', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "zQdaLnIzkYUO",
    "outputId": "0eb3585e-f7d8-4fef-bda3-f7cd455b144f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sdg_df = pd.read_csv(path + '/in/sgd_papers2.csv')\n",
    "list(sdg_df.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "dPOvl5UPk7qh",
    "outputId": "d08de571-2c94-4901-a575-f4ea8fedbe8f"
   },
   "outputs": [],
   "source": [
    "#We continue by importing the packages we'll need.\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Dropout # for MLP\n",
    "from keras.layers import LSTM, Bidirectional #,SimpleRNN For Rnn\n",
    "from keras.layers import Conv1D, MaxPooling1D # for CNN\n",
    "from keras.layers import  Input, concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "variable = sdg_df['extracted_abstract_new'].str.lower().str.replace(\"'\", '')\n",
    "\n",
    "# get the max of total words count\n",
    "sdg_df['totalwords'] = variable.str.split().str.len()\n",
    "print('max words per entry:', sdg_df['totalwords'].max())\n",
    "print('average words per entry:', sdg_df['totalwords'].median())\n",
    "print('example of one instance:', variable[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "PqmE2BDM4reE",
    "outputId": "43845588-9866-4107-fe32-b46669fb7ca0"
   },
   "outputs": [],
   "source": [
    "# Training set pre-processing\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer() #We initialize the tokenizer\n",
    "t.fit_on_texts(variable) #fit to text\n",
    "vocab_size = len(t.word_index) + 1 #here, tokenizer creates an object, named word_index, which will be our own dictionary.\n",
    "                                   #we take the length of it and add +1, just to be safe.\n",
    "print('vocab size:', vocab_size)\n",
    "\n",
    "sdg_df['encoded_var'] = t.texts_to_sequences(variable) #we insert the variable into the sdg_df, encoded\n",
    "\n",
    "max_length = int(sdg_df['totalwords'].max()) #we find the longest sentence, as it will later be used in the embeddings\n",
    "print('max padded lenght:', max_length)\n",
    "padded_var = pad_sequences(sdg_df['encoded_var'], maxlen=max_length, padding='post') #finally, we build the padded_var\n",
    "sdg_df['padded_var'] = padded_var.tolist()\n",
    "\n",
    "print('example of one instance:', padded_var[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nt-ZZmxjQZzX"
   },
   "outputs": [],
   "source": [
    "# Extract Tokenizer in csv\n",
    "\n",
    "Tokenizer = pd.DataFrame.from_dict(t.word_index, orient='index')\n",
    "Tokenizer = Tokenizer.reset_index()\n",
    "Tokenizer = Tokenizer.rename(columns={\"index\": \"Word\", 0: \"Value\"})\n",
    "Tokenizer.to_csv(path_or_buf=\"out/Tokenizer.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "rP9xB1RRsCub",
    "outputId": "2d43dd9b-d605-4b11-861f-1d017ae7a9b2"
   },
   "outputs": [],
   "source": [
    "# splitting \n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = padded_var\n",
    "\n",
    "# We use the label encoder later in testing for reverse transform!!\n",
    "le = LabelEncoder()\n",
    "le.fit(sdg_df['sdg'])\n",
    "#y = le.transform(sdg_df['sdg'])\n",
    "#y_inverse = le.inverse_transform(y)\n",
    "\n",
    "\n",
    "### Try one-hot encoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(np.asarray(sdg_df['sdg']).reshape(-1,1))\n",
    "y = enc.transform(np.asarray(sdg_df['sdg']).reshape(-1,1)).toarray() # Tranforming y\n",
    "y_inverse = enc.inverse_transform(y)\n",
    "\n",
    "print('Are indices correct between y and sdg_df column?', \n",
    "      np.array_equal(y_inverse, np.asarray(sdg_df['sdg']).reshape(-1,1)))\n",
    "\n",
    "print('Check that indices are same')\n",
    "print(sdg_df['padded_var'].head())\n",
    "print(X[:5])\n",
    "print('Check one-hot encoded y', y[:1])\n",
    "\n",
    "#Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "P2PfNxR5r5Cq",
    "outputId": "a21db5ce-aa9e-410e-d21e-f42623164c55"
   },
   "outputs": [],
   "source": [
    "# define MLP model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 64, input_length=max_length, mask_zero=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Early stopping\n",
    "checkpointer = [ModelCheckpoint(filepath= \"out/Dense.hdf5\", \n",
    "                                verbose=1, \n",
    "                                save_best_only=True),\n",
    "                  EarlyStopping(monitor='val_loss', \n",
    "                                mode='min',\n",
    "                                verbose=1,\n",
    "                                patience = 5)]\n",
    "\n",
    "# adjustet initial value of learning rate, to converge faster and avoid overfit\n",
    "opt = Adam(learning_rate=0.01)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=opt, \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model \n",
    "#commented out to preserve weights, uncomment for new train\n",
    "\n",
    "# model.fit(X_train, y_train, epochs=200, verbose=1, batch_size=12, callbacks=checkpointer, validation_split = 0.2)\n",
    "\n",
    "model.load_weights(\"out/Dense.hdf5\")\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "NbTr-RkFAOfQ",
    "outputId": "0dabc321-ca70-4ce3-ba85-2415edb47629"
   },
   "outputs": [],
   "source": [
    "#Prediction and CF\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "model.load_weights(\"out/Dense.hdf5\")\n",
    "pred = np.argmax(model.predict(X_test), axis = -1)\n",
    "\n",
    "print(confusion_matrix(pred, np.argmax(y_test, axis = -1)))\n",
    "print(classification_report(pred, np.argmax(y_test, axis = -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "FNK5oKTSMWqb",
    "outputId": "1af026e2-eb83-47a6-a88d-26285d2a2a44"
   },
   "outputs": [],
   "source": [
    "# CNN\n",
    "\n",
    "# define model\n",
    "modelCnn = Sequential()\n",
    "modelCnn.add(Embedding(vocab_size, 64, input_length=max_length, mask_zero=True))\n",
    "modelCnn.add(Conv1D(filters=32, strides=3, kernel_size=4, activation='relu'))\n",
    "#modelCnn.add(MaxPooling1D(pool_size=2))\n",
    "#modelCnn.add(Dropout(0.2))\n",
    "modelCnn.add(GlobalAveragePooling1D())\n",
    "modelCnn.add(Dense(6, activation='softmax'))\n",
    "print(modelCnn.summary())\n",
    "\n",
    "opt = Adam(learning_rate=0.01)\n",
    "\n",
    "modelCnn.compile(optimizer = opt, \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "checkpointer = [ModelCheckpoint(filepath= \"out/Cnn.hdf5\", \n",
    "                                verbose=1, \n",
    "                                save_best_only=True),\n",
    "                  EarlyStopping(monitor='val_loss', \n",
    "                                mode='min',\n",
    "                                verbose=1,\n",
    "                                patience = 5)]\n",
    "# fit the model\n",
    "\n",
    "#modelCnn.fit(X_train, y_train, epochs=50, verbose=1, batch_size=12, callbacks=checkpointer, validation_split = 0.2)\n",
    "\n",
    "# evaluate the model\n",
    "modelCnn.load_weights(\"out/Cnn.hdf5\")\n",
    "loss, accuracy = modelCnn.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "## Embedding = 64 ---------------------------------\n",
    "# maxpooling = 2\n",
    "# 64 kernel size 8 = 86\n",
    "# 32 kernel size 8 = 89\n",
    "# 64 kernel size 4 = 85\n",
    "# 32 kernel size 4 = 84\n",
    "# 16 kernel size 8 = 88\n",
    "# 16 kernel size 4 = 85\n",
    "# 16 kernel size 2 = 88\n",
    "# 12 kernel size 4 = 85\n",
    "\n",
    "# 16 kernel size 4 dense 16 = 78\n",
    "# 16 kernel size 4 dense 12 = 77\n",
    "\n",
    "# maxpooling = 4\n",
    "# 16 kernel size 4 = 88\n",
    "\n",
    "## Embedding 12 = 77\n",
    "## Embedding 24 = 84\n",
    "## Embedding 36 = 86\n",
    "## Embedding 100 = 86\n",
    "\n",
    "# lr = adam, filter =32, kernel size = 8, embeddings = 64 --> 91 initialtextnew\n",
    "\n",
    "# lr = adam, filter =32, kernel size = 8, embeddings = 64 --> 83 NormText\n",
    "\n",
    "# lr = adam, filter =32, kernel size = 8, embeddings = 64 --> 86 normalizedabstract\n",
    "\n",
    "# lr = adam, filter =32, kernel size = 8, embeddings = 64 --> 89 notNormalizedAbstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "6fH6O0S_Bst0",
    "outputId": "a5533432-66c5-4fc6-d808-4408ae9554fa"
   },
   "outputs": [],
   "source": [
    "modelCnn.load_weights(\"out/Cnn.hdf5\")\n",
    "pred = np.argmax(modelCnn.predict(X_test), axis = -1)\n",
    "\n",
    "print(confusion_matrix(pred, np.argmax(y_test, axis = -1)))\n",
    "print(classification_report(pred, np.argmax(y_test, axis = -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "K3djBKI08Ikh",
    "outputId": "8d25f3a7-dbd4-47ca-e5b5-fd1606a16f76"
   },
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "modelRnn = Sequential()\n",
    "modelRnn.add(Embedding(vocab_size, 64, input_length=max_length))\n",
    "#modelRnn.add(SpatialDropout1D(0.2))\n",
    "modelRnn.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "modelRnn.add(Dense(6, activation='softmax'))\n",
    "print(modelRnn.summary())\n",
    "\n",
    "# opt = Adam(learning_rate=0.01) not needed\n",
    "\n",
    "modelRnn.compile(optimizer='adam', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "\n",
    "checkpointer = [ModelCheckpoint(filepath= \"out/BiLstm.hdf5\", \n",
    "                                verbose=1, \n",
    "                                save_best_only=True),\n",
    "                  EarlyStopping(monitor='val_loss', \n",
    "                                mode='min',\n",
    "                                verbose=1,\n",
    "                                patience = 5)]\n",
    "# fit the model\n",
    "\n",
    "#modelRnn.fit(X_train, y_train, epochs=50, verbose=1, batch_size=12, callbacks=checkpointer, validation_split = 0.2)\n",
    "\n",
    "# evaluate the model\n",
    "modelRnn.load_weights(\"out/BiLstm.hdf5\")\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = modelRnn.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "\n",
    "#Embedding 32 lstm 16 = 80%\n",
    "\n",
    "# adjust lr=0.1\n",
    "#Embedding 64 lstm 32 = 85%\n",
    "\n",
    "#Embedding 64 lstm 32 = 88% initialtext\n",
    "#Embedding 64 lstm 32 = 88% normtext\n",
    "#Embedding 64 lstm 32 = 87% normalizedabstract\n",
    "#Embedding 64 lstm 32 = 85% NotNormalizedAbstract\n",
    "\n",
    "#Embedding 64 lstm 64 = 84% normtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "w8yDi369B0-D",
    "outputId": "f5e28e48-cbc8-4438-dc21-ace7227bee29"
   },
   "outputs": [],
   "source": [
    "modelRnn.load_weights(\"out/BiLstm.hdf5\")\n",
    "pred = np.argmax(modelRnn.predict(X_test), axis = -1)\n",
    "\n",
    "print(confusion_matrix(pred, np.argmax(y_test, axis = -1)))\n",
    "print(classification_report(pred, np.argmax(y_test, axis = -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mixed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "YDkL6DY6aVcl",
    "outputId": "a2c82670-83df-4b21-a704-9258e7bedc05"
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "text_variable = sdg_df['extracted_abstract_new'].str.lower().str.replace(\"'\", '')\n",
    "title_variable = sdg_df['extracted_title_new'].str.lower().str.replace(\"'\", '')\n",
    "\n",
    "total_words_text = text_variable.str.split().str.len()\n",
    "total_words_title = title_variable.str.split().str.len()\n",
    "\n",
    "print('max words text:', max(total_words_text))\n",
    "print('avg words text:', statistics.median(total_words_text))\n",
    "\n",
    "print('=====================')\n",
    "\n",
    "print('max words title:', max(total_words_title))\n",
    "print('avg words title:', statistics.median(total_words_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "GKzgHgZVatWO",
    "outputId": "d61af1a5-1f78-48bc-ec8d-d3a4f8f1b39d"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t1 = Tokenizer()\n",
    "t2 = Tokenizer()\n",
    "\n",
    "t1.fit_on_texts(text_variable)\n",
    "vocab_size_text = len(t1.word_index) + 1\n",
    "encoded_text = t1.texts_to_sequences(text_variable)\n",
    "max_length_text = int(max(total_words_text))\n",
    "padded_text = pad_sequences(encoded_text, maxlen=max_length_text, padding='post')\n",
    "print('max padded lenght text:', max_length_text)\n",
    "print('vocab size text:', vocab_size_text)\n",
    "\n",
    "print('====================================')\n",
    "\n",
    "t2.fit_on_texts(title_variable)\n",
    "vocab_size_title = len(t2.word_index) + 1\n",
    "encoded_title = t2.texts_to_sequences(title_variable)\n",
    "max_length_title = int(max(total_words_title))\n",
    "padded_title = pad_sequences(encoded_title, maxlen=max_length_title, padding='post')\n",
    "print('max padded lenght title:', max_length_title)\n",
    "print('vocab size title:', vocab_size_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "nu5PcFH1VWWP",
    "outputId": "b401b21a-f12e-426f-d0ce-b616b6297d85"
   },
   "outputs": [],
   "source": [
    "# Define a model function (CNN-MLP)\n",
    "\n",
    "def model():\n",
    "    # Inputs\n",
    "    input_text = Input(shape=(max_length_text))\n",
    "    input_title = Input(shape=(max_length_title))\n",
    "    \n",
    "    embed = Embedding(input_dim = vocab_size_text, output_dim = 64, input_length = max_length_text, mask_zero=True)(input_text)\n",
    "    text_mlp = Dense(64, activation='relu') (embed)\n",
    "    text_mlp = Dropout(0.2)(text_mlp)\n",
    "    text_mlp = GlobalAveragePooling1D()(text_mlp)\n",
    "    text_mlp = Model(inputs = input_text, outputs = text_mlp)\n",
    "    \n",
    "    embed = Embedding(input_dim = vocab_size_title, output_dim = 64, input_length = max_length_title, mask_zero=True)(input_title)\n",
    "    title_cnn = Conv1D(filters=64, kernel_size=8, activation='relu')(embed)\n",
    "    title_cnn = MaxPooling1D(pool_size=2)(title_cnn)\n",
    "    title_cnn = Dropout(0.2)(title_cnn)\n",
    "    title_cnn = GlobalAveragePooling1D()(title_cnn)\n",
    "    title_cnn = Model(inputs = input_title, outputs = title_cnn)\n",
    "\n",
    "    combined = concatenate([title_cnn.output, text_mlp.output])\n",
    "\n",
    "    z = Dense(6, activation='softmax')(combined)\n",
    "\n",
    "    model = Model(inputs=[text_mlp.input, title_cnn.input], outputs=z)\n",
    "\n",
    "    title_cnn.summary()\n",
    "    text_mlp.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "mixed_model = model()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "-ZxZlo9imUkU",
    "outputId": "6647a03f-0e7c-4993-8f47-aba47b19362b"
   },
   "outputs": [],
   "source": [
    "# Define a model function (CNN-LSTM)\n",
    "\n",
    "def model():\n",
    "    # Inputs\n",
    "    input_text = Input(shape=(max_length_text))\n",
    "    input_title = Input(shape=(max_length_title))\n",
    "    \n",
    "    embed = Embedding(input_dim = vocab_size_text, output_dim = 64, input_length = max_length_text, mask_zero=True)(input_text)\n",
    "    text_cnn = Conv1D(filters=64, kernel_size=8, activation='relu')(embed)\n",
    "    text_cnn = MaxPooling1D(pool_size=2)(text_cnn)\n",
    "    text_cnn = Dropout(0.2)(text_cnn)\n",
    "    text_cnn = GlobalAveragePooling1D()(text_cnn)\n",
    "    text_cnn = Model(inputs = input_text, outputs = text_cnn)\n",
    "    \n",
    "    embed = Embedding(input_dim = vocab_size_title, output_dim = 64, input_length = max_length_title, mask_zero=True)(input_title)\n",
    "    title_rnn = Bidirectional(LSTM(32, return_sequences=False)) (embed)\n",
    "    title_rnn = Model(inputs = input_title, outputs = title_rnn)\n",
    "\n",
    "\n",
    "    combined = concatenate([text_cnn.output, title_rnn.output])\n",
    "\n",
    "    z = Dense(6, activation='softmax')(combined)\n",
    "\n",
    "    model = Model(inputs=[text_cnn.input, title_rnn.input], outputs=z)\n",
    "\n",
    "    text_cnn.summary()\n",
    "    title_rnn.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "mixed_model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "McaFfFVKpLJK",
    "outputId": "7e0c3fc1-ba5e-449b-a413-52f0c813aa51"
   },
   "outputs": [],
   "source": [
    "# Define a model function (RNN - MLP)\n",
    "\n",
    "def model():\n",
    "    # Inputs\n",
    "    input_text = Input(shape=(max_length_text))\n",
    "    input_title = Input(shape=(max_length_title))\n",
    "\n",
    "    \n",
    "    embed = Embedding(input_dim = vocab_size_title, output_dim = 64, input_length = max_length_title, mask_zero=True)(input_title)\n",
    "    title_rnn = Bidirectional(LSTM(32, return_sequences=False)) (embed)\n",
    "    title_rnn = Model(inputs = input_title, outputs = title_rnn)\n",
    "    \n",
    "    embed = Embedding(input_dim = vocab_size_text, output_dim = 64, input_length = max_length_text, mask_zero=True)(input_text)\n",
    "    text_mlp = Dense(64, activation='relu') (embed)\n",
    "    text_mlp = Dropout(0.2)(text_mlp)\n",
    "    text_mlp = GlobalAveragePooling1D()(text_mlp)\n",
    "    text_mlp = Model(inputs = input_text, outputs = text_mlp)\n",
    "\n",
    "    combined = concatenate([title_rnn.output, text_mlp.output])\n",
    "\n",
    "    z = Dense(6, activation='softmax')(combined)\n",
    "\n",
    "    model = Model(inputs=[text_mlp.input, title_rnn.input], outputs=z)\n",
    "\n",
    "    title_rnn.summary()\n",
    "    text_mlp.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "mixed_model = model()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "ucffSPT3st66",
    "outputId": "893d5f3f-a618-4a3f-f108-25579426a6ca"
   },
   "outputs": [],
   "source": [
    "# Define a model function (MLP-MLP)\n",
    "\n",
    "def model():\n",
    "    # Inputs\n",
    "    input_text = Input(shape=(max_length_text))\n",
    "    input_title = Input(shape=(max_length_title))\n",
    "    \n",
    "    embed = Embedding(input_dim = vocab_size_text, output_dim = 64, input_length = max_length_text, mask_zero=True)(input_text)\n",
    "    text_mlp = Dense(64, activation='relu') (embed)\n",
    "    text_mlp = Dropout(0.2)(text_mlp)\n",
    "    text_mlp = GlobalAveragePooling1D()(text_mlp)\n",
    "    text_mlp = Model(inputs = input_text, outputs = text_mlp)\n",
    "    \n",
    "    embed = Embedding(input_dim = vocab_size_title, output_dim = 64, input_length = max_length_title, mask_zero=True)(input_title)\n",
    "    title_mlp = Dense(64, activation='relu') (embed)\n",
    "    title_mlp = Dropout(0.2)(title_mlp)\n",
    "    title_mlp = GlobalAveragePooling1D()(title_mlp)\n",
    "    title_mlp = Model(inputs = input_title, outputs = title_mlp)\n",
    "\n",
    "    combined = concatenate([text_mlp.output, title_mlp.output])\n",
    "\n",
    "    z = Dense(6, activation='softmax')(combined)\n",
    "\n",
    "    model = Model(inputs=[text_mlp.input, title_mlp.input], outputs=z)\n",
    "\n",
    "    text_mlp.summary()\n",
    "    title_mlp.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "mixed_model = model()\n",
    "\n",
    "# Dense 1 = 64, Dense 2 = 64 87\n",
    "# Dense 1 = 32, Dense 2 = 64 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "RFY74MZSsUDC",
    "outputId": "99bca11d-566b-432a-e35c-63db85acc170"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Try one-hot encoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(np.asarray(sdg_df['sdg']).reshape(-1,1))\n",
    "y = enc.transform(np.asarray(sdg_df['sdg']).reshape(-1,1)).toarray() # Tranforming y\n",
    "y_inverse = enc.inverse_transform(y)\n",
    "\n",
    "X_train_title, X_test_title, y_train_title, y_test_title = train_test_split(padded_title, y, test_size=0.3, random_state=42, stratify = y)\n",
    "\n",
    "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(padded_text, y, test_size=0.3, random_state=42, stratify = y)\n",
    "\n",
    "print('are y_train equals?:', np.array_equal(y_train_title, y_train_text))\n",
    "print('are y_test equals?:', np.array_equal(y_test_title, y_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaUW-iqvtNUB"
   },
   "outputs": [],
   "source": [
    "mixed_model.compile(optimizer='adam', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "\n",
    "checkpointer = [ModelCheckpoint(filepath= \"out/mixed_model.hdf5\", \n",
    "                                verbose=1, \n",
    "                                save_best_only=True),\n",
    "                  EarlyStopping(monitor='val_loss', \n",
    "                                mode='min',\n",
    "                                verbose=1,\n",
    "                                patience = 5)]\n",
    "\n",
    "# The section below is commented and continued in fine tuning chunk\n",
    "\n",
    "# fit the model\n",
    "#mixed_model.fit([X_train_text, X_train_title], y_train_text, epochs=200, verbose=1, batch_size=12, callbacks=checkpointer, validation_split = 0.2)\n",
    "\n",
    "# evaluate the model\n",
    "#mixed_model.load_weights(\"out/mixed_model.hdf5\")\n",
    "\n",
    "# evaluate the model\n",
    "#loss, accuracy = mixed_model.evaluate([X_test_text, X_test_title], y_test_text, verbose=1)\n",
    "#print('Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "#mixed_model.load_weights(\"out/mixed_model.hdf5\")\n",
    "#pred = np.argmax(mixed_model.predict([X_test_text, X_test_title]), axis = -1)\n",
    "\n",
    "#print(confusion_matrix(pred, np.argmax(y_test, axis = -1)))\n",
    "#print(classification_report(pred, np.argmax(y_test, axis = -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "S672yeeuZCD5",
    "outputId": "e041b963-f394-4639-d228-b4d113f0e4de"
   },
   "outputs": [],
   "source": [
    "# Fine tuning\n",
    "\n",
    "def model():\n",
    "    # Inputs\n",
    "    input_text = Input(shape=(max_length_text))\n",
    "    input_title = Input(shape=(max_length_title))\n",
    "    \n",
    "    embed = Embedding(input_dim = vocab_size_text, output_dim = 12, input_length = max_length_text)(input_text)\n",
    "    #text_mlp = Dense(18, activation='relu') (embed)\n",
    "    #text_mlp = Dropout(0.2)(text_mlp)\n",
    "    text_mlp = GlobalAveragePooling1D()(embed)\n",
    "    text_mlp = Model(inputs = input_text, outputs = text_mlp)\n",
    "    \n",
    "    embed = Embedding(input_dim = vocab_size_title, output_dim = 6, input_length = max_length_title)(input_title)\n",
    "    #title_mlp = Dense(18, activation='relu') (embed)\n",
    "    #title_mlp = Dropout(0.2)(title_mlp)\n",
    "    title_mlp = GlobalAveragePooling1D()(embed)\n",
    "    title_mlp = Model(inputs = input_title, outputs = title_mlp)\n",
    "\n",
    "    combined = concatenate([text_mlp.output, title_mlp.output])\n",
    "\n",
    "    z = Dense(6, activation='softmax')(combined)\n",
    "\n",
    "    model = Model(inputs=[text_mlp.input, title_mlp.input], outputs=z)\n",
    "\n",
    "    text_mlp.summary()\n",
    "    title_mlp.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "mixed_model = model()\n",
    "\n",
    "opt = Adam(learning_rate=0.01)\n",
    "\n",
    "mixed_model.compile(optimizer=opt, \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "\n",
    "checkpointer = [ModelCheckpoint(filepath= \"out/mixed_model.hdf5\", \n",
    "                                verbose=1, \n",
    "                                save_best_only=True),\n",
    "                  EarlyStopping(monitor='val_loss', \n",
    "                                mode='min',\n",
    "                                verbose=1,\n",
    "                                patience = 5)]\n",
    "# fit the model\n",
    "#mixed_model.fit([X_train_text, X_train_title], y_train_text, epochs=400, verbose=1, batch_size=12, callbacks=checkpointer, validation_split = 0.2)\n",
    "\n",
    "# evaluate the model\n",
    "mixed_model.load_weights(\"out/mixed_model.hdf5\")\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = mixed_model.evaluate([X_test_text, X_test_title], y_test_text, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "# 64 embedding\n",
    "# Dense 1 = 64, Dense 2 = 64 85-86\n",
    "# Dense 1 = 64, Dense 2 = 32 88-86-86\n",
    "# Dense 1 = 32, Dense 2 = 32 88-88-85 \n",
    "# Dense 1 = 32, Dense 2 = 32, lr = 0.01 86-88-87\n",
    "# Dense 1 = 32, Dense 2 = 16, lr = 0.01 86-86-85\n",
    "# Dense 1 = 16, Dense 2 = 16, lr = 0.01 86-88-86\n",
    "# No Dense lr = 0.01 87-87-86\n",
    "# No Dense lr = adam 88-87-88\n",
    "\n",
    "# 32 embedding\n",
    "# No Dense lr = adam 88-87-86\n",
    "# No Dense lr = 0.01 88-87-88\n",
    "\n",
    "# 18 embedding\n",
    "# No Dense lr = 0.01 88-88-87\n",
    "# No Dense lr = adam 88-88-87\n",
    "# Dense 1 = 18, Dense 2 = 18, lr = 0.01 82-87-88-82\n",
    "# Dense 1 = 18, Dense 2 = 18, lr = adam 83-84-84\n",
    "\n",
    "# 6 embedding\n",
    "# No Dense lr = adam 86-88-85\n",
    "# No Dense lr = 0.01  86-88-87\n",
    "\n",
    "# 3 embedding\n",
    "# No Dense lr = adam 87-88-86\n",
    "# No Dense lr = 0.01 86-86-85\n",
    "\n",
    "# 12-6 embedding\n",
    "# No Dense lr = adam 0.001 89-88-88\n",
    "# No Dense lr = 0.01 89-88-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "4JvRkpI-kSwn",
    "outputId": "c531d513-42f0-4c89-b8a2-829bc1b98fa9"
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(mixed_model, to_file='out/model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Zrs8SVzmwHr"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "W09biUX8oew9",
    "outputId": "24e962b0-86e6-47a6-f7b9-f7ad6501dddb"
   },
   "outputs": [],
   "source": [
    "# Importing test set from zip\n",
    "#https://stackoverflow.com/questions/31357930/multiple-text-files-analysis-in-a-zip\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "zip_file = ZipFile('in/test_set.zip')\n",
    "text_files = zip_file.infolist()\n",
    "\n",
    "\n",
    "df1, df2 = {}, {}\n",
    "i = 0\n",
    "\n",
    "for fname in text_files:\n",
    "  df1[i] = zip_file.read(fname)\n",
    "  df2[i] = fname.filename.replace('test_set/', \"\")\n",
    "  i = i + 1\n",
    "\n",
    "mydicts = [df2, df1]\n",
    "\n",
    "test_df = pd.concat([pd.Series(d) for d in mydicts], axis=1) #concat \n",
    "test_df.columns = ['Title', 'Text'] # name columns\n",
    "test_df = test_df.iloc[1:].reset_index(drop=True) # remove first row because its empty\n",
    "test_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "xgC18iETgA-R",
    "outputId": "5bd66baa-c5d0-4c8b-9f72-f197e9fa1094"
   },
   "outputs": [],
   "source": [
    "# Test set pre-processing\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(u'stopwords')\n",
    "\n",
    "# Some simple pre-processing\n",
    "test_df[\"Text_new\"] = test_df[\"Text\"].astype(np.str).str[1:] # remove b char from text\n",
    "test_df[\"Text_new\"] = test_df[\"Text_new\"].str.replace('[^\\w\\s]', \"\") # remove symbols\n",
    "\n",
    "#Token splitting\n",
    "test_df['Text_new_token'] = test_df.apply(lambda row: nltk.word_tokenize(row['Text_new']), axis=1)\n",
    "\n",
    "# Remove stopwords\n",
    "stops = set(stopwords.words('english'))  #make a list, \"stop\", with english stopwords\n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['Text_new_token']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "test_df['Extracted_text_new'] = test_df.apply(remove_stops, axis=1)\n",
    "\n",
    "# Tokenize with pretrained t to numbers\n",
    "test_df['Encoded_var'] = t.texts_to_sequences(test_df[\"Extracted_text_new\"])\n",
    "\n",
    "print('test a sentence:', test_df['Extracted_text_new'].head(1))\n",
    "print('test encoding worked:', test_df['Encoded_var'].head(1))\n",
    "print('max padded lenght from training:', max_length)\n",
    "\n",
    "# Padding\n",
    "padded_var_test = np.asarray(pad_sequences(test_df['Encoded_var'], maxlen=max_length, padding='post'))\n",
    "test_df['Padded_var'] = padded_var_test.tolist()\n",
    "\n",
    "print('example of one instance in test:', padded_var_test[0])\n",
    "\n",
    "# Find out the difference in count of words between trained data and test data\n",
    "test_df['Total_words'] = test_df['Extracted_text_new'].str.len()\n",
    "print('max words per entry in test:', test_df['Total_words'].max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 804
    },
    "id": "0AGozXYOYvl7",
    "outputId": "9cf80c3d-26c9-45ec-ccc4-bf817bf6ccf4"
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "KbaCOqUcmyh9",
    "outputId": "d9b801c9-f05b-4c68-cd9c-55de80e85033"
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "# https://www.machinecurve.com/index.php/2020/02/21/how-to-predict-new-samples-with-your-keras-model/\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 64, input_length=max_length, mask_zero=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.load_weights(\"out/Dense.hdf5\")\n",
    "pred = np.argmax(model.predict(padded_var_test, batch_size=1), axis = -1)\n",
    "test_df['Prediction'] = le.inverse_transform(pred)\n",
    "\n",
    "#To_csv\n",
    "test_df[['Title','Prediction']].to_csv(path_or_buf=\"out/group2_sdg_predictions.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
